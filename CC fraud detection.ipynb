{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data  processing, CSV file\n",
    "import sys # system-spesific parameters and functions\n",
    "from sklearn.preprocessing import StandardScaler # Scale the feautres\n",
    "import imblearn # Handling Imballance data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier # This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "from sklearn.feature_selection import SelectFromModel # Meta-transformer for selecting features based on importance weights.\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import seaborn as sns # visualise random distributions. It uses matplotlb\n",
    "\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('creditcard.csv', header = 0, comment='\\t', sep = \",\")\n",
    "\n",
    "\n",
    "\n",
    "### Data Exploration ###\n",
    "\n",
    "\n",
    "\n",
    "# read the first five rows\n",
    "dataset.head() \n",
    "#check out the dimension of the dataset\n",
    "dataset.shape \n",
    "\n",
    "# Obervations:\n",
    "    # Interesting info (memory_usage, null_counts = 0)\n",
    "    # We see that we have only numerical values so no need to transform categorical ones into dummy variables and also non-null values\n",
    "\n",
    "# Print the full summary and the columns \n",
    "dataset.info() \n",
    "dataset.columns\n",
    "\n",
    "## Desrriptive Statistics\n",
    "\n",
    "\n",
    "# Summarize the central tendency, dispersion and shape of a datasetâ€™s distribution, excluding NaN values.\n",
    "dataset.describe()\n",
    "# As most of the columns V1, V2,... V28 are transformed using PCA so neither features make much sense and nor will the descriptive statistics so we will leave them and consider only Time and Amount which makes sense. \n",
    "dataset[['Time', 'Amount']].describe()\n",
    "\n",
    "# Observations:\n",
    "    # Mean transaction is somewhere is 88 and standard deviation is around 250.\n",
    "    # The median is 22 which is very less as compared to mean which signifies that there are outliers or our data is highly positive skewed which is effecting the amount and thus the mean. \n",
    "    # The maximum transaction that was done is of 25,691 and minimum is 0.\n",
    "\n",
    "\n",
    "# Check the percentages of fraudulent and non-fraudulent transactions\n",
    "majority, minority = np.bincount(dataset['Class'])\n",
    "total = majority + minority\n",
    "\n",
    "\n",
    "\n",
    "print('Examples:\\n    Total: {}\\n    Minority: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, minority, 100 * minority / total))\n",
    "print(f'Percent of Non-Fraudulent Transactions(Majority) = {round(dataset[\"Class\"].value_counts()[0]/len(dataset) * 100,2)}%') # \n",
    "print(f'Percent of Fraudulent Transactions(Minority) = {round(dataset[\"Class\"].value_counts()[1]/len(dataset) * 100,2)}%')\n",
    "      \n",
    "\n",
    "      \n",
    "      # Feature Correlation with Response to the label(Class)\n",
    "corr = dataset.corrwith(dataset['Class']).reset_index()\n",
    "corr.columns = ['Index','Correlations']\n",
    "corr = corr.set_index('Index')\n",
    "corr = corr.sort_values(by=['Correlations'], ascending = True)\n",
    "plt.figure(figsize=(9, 12))\n",
    "fig = sns.heatmap(corr, annot=True, fmt=\"g\", cmap='Set3', linewidths=0.3, linecolor='black')\n",
    "plt.title(\"Feature Correlation with Class\", fontsize=18)\n",
    "plt.show()\n",
    "      \n",
    "      \n",
    "      \n",
    "      # Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "scaled_dataset = dataset.copy()\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "scaled_dataset ['scaled_amount'] = std_scaler.fit_transform(scaled_dataset ['Amount'].values.reshape(-1,1))\n",
    "scaled_dataset ['scaled_time'] = std_scaler.fit_transform(scaled_dataset ['Time'].values.reshape(-1,1))\n",
    "\n",
    "scaled_dataset .drop(['Time','Amount'], axis=1, inplace=True)\n",
    "scaled_amount = scaled_dataset ['scaled_amount']\n",
    "scaled_time = scaled_dataset ['scaled_time']\n",
    "\n",
    "scaled_dataset .drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "scaled_dataset .insert(0, 'scaled_amount', scaled_amount)\n",
    "scaled_dataset .insert(1, 'scaled_time', scaled_time)\n",
    "print(scaled_dataset.describe())\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      # Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "scaled_dataset = dataset.copy()\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "scaled_dataset ['scaled_amount'] = std_scaler.fit_transform(scaled_dataset ['Amount'].values.reshape(-1,1))\n",
    "scaled_dataset ['scaled_time'] = std_scaler.fit_transform(scaled_dataset ['Time'].values.reshape(-1,1))\n",
    "\n",
    "scaled_dataset .drop(['Time','Amount'], axis=1, inplace=True)\n",
    "scaled_amount = scaled_dataset ['scaled_amount']\n",
    "scaled_time = scaled_dataset ['scaled_time']\n",
    "\n",
    "scaled_dataset .drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "scaled_dataset .insert(0, 'scaled_amount', scaled_amount)\n",
    "scaled_dataset .insert(1, 'scaled_time', scaled_time)\n",
    "print(scaled_dataset.describe())\n",
    "      \n",
    "      \n",
    "      \n",
    "      # Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "scaled_dataset = dataset.copy()\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "scaled_dataset ['scaled_amount'] = std_scaler.fit_transform(scaled_dataset ['Amount'].values.reshape(-1,1))\n",
    "scaled_dataset ['scaled_time'] = std_scaler.fit_transform(scaled_dataset ['Time'].values.reshape(-1,1))\n",
    "\n",
    "scaled_dataset .drop(['Time','Amount'], axis=1, inplace=True)\n",
    "scaled_amount = scaled_dataset ['scaled_amount']\n",
    "scaled_time = scaled_dataset ['scaled_time']\n",
    "\n",
    "scaled_dataset .drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "scaled_dataset .insert(0, 'scaled_amount', scaled_amount)\n",
    "scaled_dataset .insert(1, 'scaled_time', scaled_time)\n",
    "print(scaled_dataset.describe())\n",
    "      \n",
    "      \n",
    "      \n",
    "      # Build the Support Vector Machines model\n",
    "\n",
    "# Build and calculate the classifier's process\n",
    "start = time.time()\n",
    "clfSVM = svm.SVC()\n",
    "clfSVM.fit(X_trainfinal, Y_trainnew)  \n",
    "y_predSVM = clfSVM.predict(X_testfinal)\n",
    "end = time.time()\n",
    "print(\"This is the time of SVM = \", end - start)\n",
    "\n",
    "# Performance Metrics\n",
    "print('Support Vector Machines Metrics-Score:')\n",
    "#Confusion Matrix\n",
    "confusion_matrix4 = confusion_matrix(y_test, y_predSVM)\n",
    "print(\"\t\", \"pred no\", \"pred yes\")\n",
    "print(\"actual no\", confusion_matrix4[0])  \n",
    "print(\"actual yes\", confusion_matrix4[1])\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy_SVM = accuracy_score(y_test, y_predSVM)\n",
    "print('Accuracy: %f' % accuracy_SVM)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision_SVM = precision_score(y_test, y_predSVM)\n",
    "print('Precision: %f' % precision_SVM)\n",
    "\n",
    "#sys.exit()\n",
    "# recall: tp / (tp + fn)\n",
    "recall_SVM = recall_score(y_test, y_predSVM)\n",
    "print('Recall: %f' % recall_SVM)\n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1_SVM = f1_score(y_test, y_predSVM)\n",
    "print('F1 score: %f' % f1_SVM)\n",
    "\n",
    "#AUC_ROC\n",
    "auc_SVM = roc_auc_score(y_test, y_predSVM)\n",
    "print('ROC_AUC score: ', roc_auc_score(y_test, y_predSVM))\n",
    "\n",
    "# Classification report\n",
    "labels = ['No Fraud', 'Fraud']\n",
    "print(classification_report(y_test, y_predSVM, target_names=labels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# All ROC_AUC scores\n",
    "\n",
    "print('Logistic Regression ROC_AUC Score: ', roc_auc_score(y_test, y_predLR))\n",
    "print('Random Forests ROC_AUC Score: ', roc_auc_score(y_test, y_predRF))\n",
    "print('Naive Bayes ROC_AUC Score : ', roc_auc_score(y_test, y_predNB))\n",
    "print('Support Vector Machines ROC_AUC Score: ', roc_auc_score(y_test, y_predSVM))\n",
    "\n",
    "log_fpr, log_tpr, log_thresold = roc_curve(y_test, y_predLR)\n",
    "rf_fpr, rf_tpr, rf_threshold = roc_curve(y_test, y_predRF)\n",
    "nb_fpr, nb_tpr, nb_threshold = roc_curve(y_test, y_predNB)\n",
    "svm_fpr, svm_tpr, svm_threshold = roc_curve(y_test, y_predSVM)\n",
    "#ab_fpr, ab_tpr, ab_threshold = roc_curve(y_test, y_predAB)\n",
    "\n",
    "def graph_roc_curve_multiple(log_fpr, log_tpr, rf_fpr, rf_tpr, nb_fpr, nb_tpr, svm_fpr, svm_tpr):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('ROC Curve \\n Random Forests have the highest score', fontsize=18)\n",
    "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Score: {:.4f}'.format(roc_auc_score(y_test, y_predLR)))\n",
    "    plt.plot(rf_fpr, rf_tpr, label='Random Forests Score: {:.4f}'.format(roc_auc_score(y_test, y_predRF)))\n",
    "    plt.plot(nb_fpr, nb_tpr, label='Naive Bayes Score: {:.4f}'.format(roc_auc_score(y_test, y_predNB)))\n",
    "    plt.plot(svm_fpr, svm_tpr, label='Support Vector Machines Score: {:.4f}'.format(roc_auc_score(y_test, y_predSVM)))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.01, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
    "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
    "                )\n",
    "    plt.legend()\n",
    "\n",
    "graph_roc_curve_multiple(log_fpr, log_tpr, rf_fpr, rf_tpr, nb_fpr, nb_tpr, svm_fpr, svm_tpr)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "      \n",
    "      \n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
