{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dcb3740c077f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;31m# system-spesific parameters and functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m \u001b[1;31m# Scale the feautres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mimblearn\u001b[0m \u001b[1;31m# Handling Imballance data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m \u001b[1;31m# Resampling Technique\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "# Clearifications:\n",
    "\n",
    "    \n",
    "# Confusion Matrix and Performance Metrics:\n",
    "    # True negatives and true positives are samples that were correctly classified\n",
    "    # False negatives and false positives are samples that were incorrectly classified\n",
    "        # Legitimate Transactions Detected (True Negatives(TN) = pred no and actual no))\n",
    "        # Fraudulent Transactions Detected (True Positives(TP) = pred yes and actual yes))\n",
    "        # Fraudulent Transactions Missed (False Negatives(FN) = pred no and actual yes))\n",
    "        # Legitimate Transactions Incorrectly Detected (False Positives(FP) = pred yes/actual no))\n",
    "\n",
    "    \n",
    "    # Accuracy is the percentage of examples correctly classified (tp + tn) / (p + n))\n",
    "    # Precision is the percentage of predicted positives that were correctly classified  (tp / (tp + fp)\n",
    "        # A low precision score is indicative of a high number of false positives.\n",
    "    # Recall is the percentage of actual positives that were correctly classified (tp / (tp + fn))\n",
    "        # A low recall score is indicative of a high number of false negatives\n",
    "    # F1 score combines precision and recall of a class in one metric. (2 tp / (2 tp + fp + fn))\n",
    "    # AUC refers to the Area Under the Curve of a Receiver Operating Characteristic curve (ROC-AUC)\n",
    "        # This metric is equal to the probability that a classifier will rank a random positive sample higher than a random negative sample\n",
    "        # They provide an overly optimistic picture of the model skill on imbalanced data \n",
    "        # This happens because the ROC curve is constructed using the number of true negatives in the 'False Positive Rate' calculation\n",
    "   \n",
    "\n",
    "# Suggestons:\n",
    "    # high recall + high precision : the class is perfectly handled by the model\n",
    "    # low recall + high precision : the model can’t detect the class well but is highly trustable when it does\n",
    "    # high recall + low precision : the class is well detected but the model also include points of other classes in it\n",
    "    # low recall + low precision : the class is poorly handled by the model\n",
    "\n",
    "\n",
    "# Why accuracy is ΝΟΤ such a good metric in our case and why f1 is needed - Due to the imbalanced data\n",
    "    # Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial\n",
    "    # Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes\n",
    "    # In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric to evaluate our model on\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Import libraries and packages essential for the code \"\"\"\n",
    "\n",
    "\n",
    "# Main Libraries for analysis\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data  processing, CSV file\n",
    "import sys # system-spesific parameters and functions\n",
    "from sklearn.preprocessing import StandardScaler # Scale the feautres\n",
    "import imblearn # Handling Imballance data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler # Resampling Technique\n",
    "# from imblearn.over_sampling import RandomOverSampler # Resampling Technique\n",
    "# from imblearn.over_sampling import SMOTE  # Resampling Technique\n",
    "# from imblearn.combine import SMOTETomek  # Resampling Technique\n",
    "# from imblearn.under_sampling import TomekLinks # Resampling Technique\n",
    "# from imblearn.over_sampling import ADASYN  # Resampling Technique\n",
    "\n",
    "\n",
    "# Classifiers and Modeling Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Features Importances - Selection Libraries\n",
    "from sklearn.ensemble import ExtraTreesClassifier # This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "from sklearn.feature_selection import SelectFromModel # Meta-transformer for selecting features based on importance weights.\n",
    "\n",
    "\n",
    "# Performance Metrics and Visualisations\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import seaborn as sns # visualise random distributions. It uses matplotlb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# read the data using the pandas library\n",
    "dataset = pd.read_csv('creditcard.csv', header = 0, comment='\\t', sep = \",\")\n",
    "\n",
    "\n",
    "\n",
    "### Data Exploration ###\n",
    "\n",
    "\n",
    "\n",
    "# read the first five rows\n",
    "dataset.head() \n",
    "#check out the dimension of the dataset\n",
    "dataset.shape \n",
    "\n",
    "# Obervations:\n",
    "    # Interesting info (memory_usage, null_counts = 0)\n",
    "    # We see that we have only numerical values so no need to transform categorical ones into dummy variables and also non-null values\n",
    "\n",
    "# Print the full summary and the columns \n",
    "dataset.info() \n",
    "dataset.columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Desrriptive Statistics\n",
    "\n",
    "\n",
    "# Summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values.\n",
    "dataset.describe()\n",
    "# As most of the columns V1, V2,... V28 are transformed using PCA so neither features make much sense and nor will the descriptive statistics so we will leave them and consider only Time and Amount which makes sense. \n",
    "dataset[['Time', 'Amount']].describe()\n",
    "\n",
    "# Observations:\n",
    "    # Mean transaction is somewhere is 88 and standard deviation is around 250.\n",
    "    # The median is 22 which is very less as compared to mean which signifies that there are outliers or our data is highly positive skewed which is effecting the amount and thus the mean. \n",
    "    # The maximum transaction that was done is of 25,691 and minimum is 0.\n",
    "\n",
    "\n",
    "# Check the percentages of fraudulent and non-fraudulent transactions\n",
    "majority, minority = np.bincount(dataset['Class'])\n",
    "total = majority + minority\n",
    "\n",
    "\n",
    "\n",
    "print('Examples:\\n    Total: {}\\n    Minority: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, minority, 100 * minority / total))\n",
    "print(f'Percent of Non-Fraudulent Transactions(Majority) = {round(dataset[\"Class\"].value_counts()[0]/len(dataset) * 100,2)}%') # \n",
    "print(f'Percent of Fraudulent Transactions(Minority) = {round(dataset[\"Class\"].value_counts()[1]/len(dataset) * 100,2)}%')\n",
    "\n",
    "\n",
    "# Observations:\n",
    "    # Only 492 (or 0.17%) of transaction are fraudulent. That means the data is highly unbalanced with respect with target variable Class.\n",
    "    # Most of the transactions are legitimate. In case we use this data to predtict the frauds, our algorithms will overfit. There will be a bias towards the majority class and the accuracy of the models will be misleading. \n",
    "    # So, later on, we will balance the data to make the algorithms to produce reliable results.\n",
    "\n",
    "\n",
    "\n",
    "# Feature Correlation with Response to the label(Class)\n",
    "corr = dataset.corrwith(dataset['Class']).reset_index()\n",
    "corr.columns = ['Index','Correlations']\n",
    "corr = corr.set_index('Index')\n",
    "corr = corr.sort_values(by=['Correlations'], ascending = True)\n",
    "plt.figure(figsize=(9, 12))\n",
    "fig = sns.heatmap(corr, annot=True, fmt=\"g\", cmap='Set3', linewidths=0.3, linecolor='black')\n",
    "plt.title(\"Feature Correlation with Class\", fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Observations:\n",
    "    # V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.\n",
    "    # V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction.\n",
    "    # For some of the features we can observe a good selectivity in terms of distribution for the two values of Class: V4, V11 have clearly separated distributions for Class values 0 and 1,\n",
    "    # V12, V14, V18 are partially separated, V1, V2, V3, V10 have a quite distinct profile, whilst V20-V28 have similar profiles for the two values of Class and thus not very useful in differentiation of both the classes.\n",
    "    # In general, with just few exceptions (Time and Amount), the features distribution for legitimate transactions (values of Class = 0) is centered around 0, sometime with a long queue at one of the extremities. \n",
    "    # In the same time, the fraudulent transactions (values of Class = 1) have a skewed (asymmetric) distributio.\n",
    "\n",
    "\n",
    "\n",
    "# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "scaled_dataset = dataset.copy()\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "scaled_dataset ['scaled_amount'] = std_scaler.fit_transform(scaled_dataset ['Amount'].values.reshape(-1,1))\n",
    "scaled_dataset ['scaled_time'] = std_scaler.fit_transform(scaled_dataset ['Time'].values.reshape(-1,1))\n",
    "\n",
    "scaled_dataset .drop(['Time','Amount'], axis=1, inplace=True)\n",
    "scaled_amount = scaled_dataset ['scaled_amount']\n",
    "scaled_time = scaled_dataset ['scaled_time']\n",
    "\n",
    "scaled_dataset .drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "scaled_dataset .insert(0, 'scaled_amount', scaled_amount)\n",
    "scaled_dataset .insert(1, 'scaled_time', scaled_time)\n",
    "print(scaled_dataset.describe())\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "### Data Manipulation ###\n",
    "\n",
    "\n",
    "\n",
    "# Before proceeding with the Random UnderSampling technique we have to separate the orginal dataframe.\n",
    "# We do this because we want to test our models on the original testing set and not on the testing set created by the Random UnderSampling technique.\n",
    "# Also, the resampling technique should be done only on the training set. \n",
    "\n",
    "\n",
    "\n",
    "# Data Split for training 80:20\n",
    "X = scaled_dataset.drop(['Class'], axis=1) # Features\n",
    "Y = scaled_dataset['Class'] # Labels\n",
    "# The tes_size is being chosen by general rule\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Check the shape\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Resampling Technique - UNDERSAMPLING - Balance the data - Handling imbalanced data Process\n",
    "\n",
    "\n",
    "# We need ratio = 1 between the two classes\n",
    "undersample = RandomUnderSampler(sampling_strategy=1, random_state=42) \n",
    "X_trainundersam, y_trainundersam = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Returning to new training set # Concat. # Concatenate pandas objects along a particular axis with optional set logic along the other axes. Can also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number.\n",
    "undersamdataset = pd.concat([X_trainundersam, y_trainundersam.reindex(X_trainundersam.index)], axis=1)\n",
    "\n",
    "\n",
    "# equally distributed\n",
    "print('Distribution of the Classes in the Undersampling subsample dataset')\n",
    "print(undersamdataset['Class'].value_counts()/len(undersamdataset))\n",
    "\n",
    "# Check the difference\n",
    "print(undersamdataset) \n",
    "print(dataset) \n",
    "\n",
    "\n",
    "# Separate undersampled data into X and y sets - split features and labels \n",
    "X_trainnew = undersamdataset.drop(['Class'], axis=1)  # Features\n",
    "Y_trainnew = undersamdataset[\"Class\"] # Mono ta lables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Resampling Technique - OVERSAMPLING - Balance the data - Handling imbalanced data Process\n",
    "ros = RandomOverSampler(sampling_strategy=1, random_state=42)\n",
    "X_trainoversam, y_trainoversam = ros.fit_resample(X_train, y_train)\n",
    "# Returning to new training set # Concat. # Concatenate pandas objects along a particular axis with optional set logic along the other axes. Can also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number.\n",
    "oversamdataset = pd.concat([X_trainoversam, y_trainoversam.reindex(X_trainoversam.index)], axis=1)\n",
    "# check the distribution\n",
    "print('Distribution of the Classes in the oversampling subsample dataset')\n",
    "print(oversamdataset['Class'].value_counts()/len(oversamdataset))\n",
    "# check the difference\n",
    "print(oversamdataset)\n",
    "print(dataset)\n",
    "# Resampling Technique - SMOTE - Balance the data - Handling imbalanced data Process\n",
    "sm = SMOTE(random_state=42)\n",
    "X_trainsmote, y_trainsmote = sm.fit_resample(X_train, y_train)\n",
    "# check the distribution\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_trainsmote.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_trainsmote.shape)) \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_trainsmote == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_trainsmote == 0))) \n",
    "# Returning to new training set # Concat. # Concatenate pandas objects along a particular axis with optional set logic along the other axes. Can also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number.\n",
    "smote = pd.concat([pd.DataFrame(X_trainsmote), pd.DataFrame(y_trainsmote)], axis=1)\n",
    "# equally distributed\n",
    "print('Distribution of the Classes in the SMOTE subsample dataset')\n",
    "print(smote['Class'].value_counts()/len(smote))\n",
    "# check the difference\n",
    "print(smote)\n",
    "print(dataset)\n",
    "# Separate SMOTE data into X and y sets - split features and labels \n",
    "X_trainnew = smote.drop(['Class'], axis=1)  # Features\n",
    "print(X_trainnew)\n",
    "Y_trainnew = smote[\"Class\"] # Mono ta lables\n",
    "print(Y_trainnew)\n",
    "# Resampling Technique - SMOTETomek - Balance the data - Handling imbalanced data Process\n",
    "smtomek = SMOTETomek()\n",
    "X_smtomek, y_smtomek = smtomek.fit_sample(X_train, y_train)\n",
    "# Returning to new training set # Concat. # Concatenate pandas objects along a particular axis with optional set logic along the other axes. Can also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number.\n",
    "smotetomek = pd.concat([pd.DataFrame(X_smtomek), pd.DataFrame(y_smtomek)], axis=1)\n",
    "#check the distribution\n",
    "print('Distribution of the Classes in the SMOTETomek subsample dataset')\n",
    "print(smotetomek['Class'].value_counts()/len(smotetomek))\n",
    "# check the difference\n",
    "print(smotetomek)\n",
    "print(dataset)\n",
    "# Separate undersampled data into X and y sets - lit labels and features - Getting the features and labels(train and labesl) - Upodhlwnw ta features kai labels opou me auta tha ekpaideusw to modelo mou\n",
    "X_trainnew = smotetomek.drop(['Class'], axis=1)  # Features\n",
    "print(X_trainnew)\n",
    "Y_trainnew = smotetomek[\"Class\"] # Mono ta lables\n",
    "print(Y_trainnew)\n",
    "# Resampling Technique - Balance the data - Handling imbalanced data Process\n",
    "# TomekLinks undersampling. Exist if the two samples are the nearest neighbors of each other. \n",
    "# Only remove samples form the majority class\n",
    "tl = TomekLinks()\n",
    "X_tl, y_tl  = tl.fit_sample(X_train, y_train)\n",
    "# Returning to new training set # Concat. # Concatenate pandas objects along a particular axis with optional set logic along the other axes. Can also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number.\n",
    "tldataset = pd.concat([X_tl, y_tl.reindex(X_tl.index)], axis=1)\n",
    "# Check the distribution\n",
    "print('Distribution of the Classes in the TomekLinks subsample dataset')\n",
    "print(tldataset['Class'].value_counts()/len(tldataset))\n",
    "# Check the difference\n",
    "print(tldataset) \n",
    "print(dataset) \n",
    "# Separate undersampled data into X and y sets - split features and labels \n",
    "X_trainnew = tldataset.drop(['Class'], axis=1)  # Features\n",
    "Y_trainnew = tldataset[\"Class\"] # Mono ta lables\n",
    "# Resampling Technique - ADASYN - Balance the data - Handling imbalanced data Process\n",
    "ada = ADASYN(sampling_strategy=1, random_state=42)\n",
    "X_trainadasyn, y_trainadasyn = ada.fit_resample(X_train, y_train)\n",
    "# Returning to new training set # Concat. # Concatenate pandas objects along a particular axis with optional set logic along the other axes. Can also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number.\n",
    "adasyndataset = pd.concat([X_trainadasyn, y_trainadasyn.reindex(X_trainadasyn.index)], axis=1)\n",
    "# check the distribution\n",
    "print('Distribution of the Classes in the ADASYN subsample dataset')\n",
    "print(adasyndataset['Class'].value_counts()/len(adasyndataset))\n",
    "# Check the difference\n",
    "print(adasyndataset) \n",
    "print(dataset) \n",
    "# Separate undersampled data into X and y sets - split features and labels \n",
    "X_trainnew = adasyndataset.drop(['Class'], axis=1)  # Features\n",
    "Y_trainnew = adasyndataset[\"Class\"] # Mono ta lables\n",
    "\"\"\"\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "### Feature Selection ###\n",
    "\n",
    "\n",
    "\n",
    "# Selecting features with the ExtraTressClassifier and SelectFromModel.\n",
    "# Note: ExtraTreesClassifier tends to be biased. But This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "etcmodel = ExtraTreesClassifier(n_estimators=100, criterion = 'entropy', random_state=42)\n",
    "etcmodel.fit(X_trainnew, Y_trainnew)\n",
    "feat_labels = X_trainnew.columns.values\n",
    "#print(feat_labels)\n",
    "feat_import = etcmodel.feature_importances_\n",
    "#print(feat_import)\n",
    "\n",
    "importances = etcmodel.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in etcmodel.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_trainnew.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "\n",
    "# Plot the impurity-based feature importances of the ExtraTreesClassifier\n",
    "plt.figure()\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Features Importance\")\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_trainnew.shape[1]), importances[indices],\n",
    "        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_trainnew.shape[1]), indices)\n",
    "plt.xlim([-1, X_trainnew.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Select the most important Values\n",
    "# We will use SelectFromModel, using a threshold to extract the most important features\n",
    "# Setting the threshold for which variables to keep based on their variance\n",
    "\n",
    "\n",
    "\n",
    "sfm = SelectFromModel(etcmodel, threshold=0.03, prefit=True)\n",
    "print('Number of features before selection: {}'.format(X_trainnew.shape[1]))\n",
    "# Number of features before selection: 30\n",
    "\n",
    "# Throwing away all the variables which fall below the threshold level␣,→specified above\n",
    "n_features = sfm.transform(X_trainnew).shape[1]\n",
    "print('Number of features after selection: {}'.format(n_features))\n",
    "# Number of features after selection: 10\n",
    "\n",
    "#Create a kist\n",
    "selected_features = list(feat_labels[sfm.get_support()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# split features and labels adding only the selected features\n",
    "X_trainfinal = undersamdataset[selected_features]\n",
    "X_testfinal = X_test[selected_features]\n",
    "\n",
    "\n",
    "#check the difference\n",
    "print(X_trainfinal)\n",
    "print(X_trainnew)\n",
    "\n",
    "# The training and testing should have the same features\n",
    "print(X_trainfinal.columns)\n",
    "print(X_testfinal.columns)\n",
    "#check the difference\n",
    "print(X_testfinal)\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# Train datasets: X_trainfinal, Y_trainnew\n",
    "# Test datasets:  X_testfinal, y_test\n",
    "\n",
    "\n",
    "\n",
    "### Data Modeling ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build the Logistic Regression model\n",
    "\n",
    "# Build and calculate the classifier's process\n",
    "start = time.time()\n",
    "clfLR = LogisticRegression()\n",
    "clfLR.fit(X_trainfinal, Y_trainnew)\n",
    "y_predLR = clfLR.predict(X_testfinal)\n",
    "end = time.time()\n",
    "print(\"This is the time of LR = \", end - start)\n",
    "\n",
    "\n",
    "# Performance Metrics\n",
    "print('Logistic Regression Metrics-Score:')\n",
    "#Confusion Matrix\n",
    "confusion_matrix1 = confusion_matrix(y_test, y_predLR)\n",
    "print(\"\t\", \"pred no\", \"pred yes\")\n",
    "print(\"actual no\", confusion_matrix1[0])  \n",
    "print(\"actual yes\", confusion_matrix1[1])\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy_LR = accuracy_score(y_test, y_predLR)\n",
    "print('Accuracy: %f' % accuracy_LR)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision_LR = precision_score(y_test, y_predLR)\n",
    "print('Precision: %f' % precision_LR)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall_LR = recall_score(y_test, y_predLR)\n",
    "print('Recall: %f' % recall_LR)\n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1_LR = f1_score(y_test, y_predLR)\n",
    "print('F1 score: %f' % f1_LR)\n",
    "\n",
    "#AUC_ROC\n",
    "auc_LR = roc_auc_score(y_test, y_predLR)\n",
    "print('ROC_AUC score: ', roc_auc_score(y_test, y_predLR))\n",
    "\n",
    "# Classification report\n",
    "labels = ['No Fraud', 'Fraud']\n",
    "print(classification_report(y_test, y_predLR, target_names=labels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Built Naive Bayes model\n",
    "\n",
    "# Build and calculate the classifier's process\n",
    "start = time.time()\n",
    "clfNB = GaussianNB()\n",
    "clfNB.fit(X_trainfinal, Y_trainnew) \n",
    "y_predNB = clfNB.predict(X_testfinal)\n",
    "end = time.time()\n",
    "print(\"This is the time of NB = \", end - start)\n",
    "\n",
    "\n",
    "# Performance Metrics\n",
    "print('Naive Bayes Metrics-Score:')\n",
    "#Confusion Matrix\n",
    "confusion_matrix2 = confusion_matrix(y_test, y_predNB)\n",
    "print(\"\t\", \"pred no\", \"pred yes\")\n",
    "print(\"actual no\", confusion_matrix2[0])  \n",
    "print(\"actual yes\", confusion_matrix2[1])\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy_NB = accuracy_score(y_test, y_predNB)\n",
    "print('Accuracy: %f' % accuracy_NB)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision_NB = precision_score(y_test, y_predNB)\n",
    "print('Precision: %f' % precision_NB)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall_NB = recall_score(y_test, y_predNB)\n",
    "print('Recall: %f' % recall_NB)\n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1_NB = f1_score(y_test, y_predNB)\n",
    "print('F1 score: %f' % f1_NB)\n",
    "\n",
    "#AUC_ROC\n",
    "auc_NB = roc_auc_score(y_test, y_predNB)\n",
    "print('ROC_AUC score: ', roc_auc_score(y_test, y_predNB))\n",
    "\n",
    "# Classification report\n",
    "labels = ['No Fraud', 'Fraud']\n",
    "print(classification_report(y_test, y_predNB, target_names=labels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build Random Forest model\n",
    "\n",
    "# Build and calculate the classifier's process\n",
    "start = time.time()\n",
    "clfRF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clfRF.fit(X_trainfinal, Y_trainnew) \n",
    "y_predRF = clfRF.predict(X_testfinal)\n",
    "end = time.time()\n",
    "print(\"This is the time of RF = \", end - start)\n",
    "\n",
    "\n",
    "# Performance Metrics\n",
    "print('Random Forests Metrics-Score:')\n",
    "#Confusion Matrix\n",
    "confusion_matrix3 = confusion_matrix(y_test, y_predRF)\n",
    "print(\"\t\", \"pred no\", \"pred yes\")\n",
    "print(\"actual no\", confusion_matrix3[0])  \n",
    "print(\"actual yes\", confusion_matrix3[1])\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy_RF = accuracy_score(y_test, y_predRF)\n",
    "print('Accuracy: %f' % accuracy_RF)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision_RF = precision_score(y_test, y_predRF)\n",
    "print('Precision: %f' % precision_RF)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall_RF = recall_score(y_test, y_predRF)\n",
    "print('Recall: %f' % recall_RF)\n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1_RF = f1_score(y_test, y_predRF)\n",
    "print('F1 score: %f' % f1_RF)\n",
    "\n",
    "#AUC_ROC\n",
    "auc_RF = roc_auc_score(y_test, y_predRF)\n",
    "print('ROC_AUC score: ', roc_auc_score(y_test, y_predRF))\n",
    "\n",
    "# Classification report\n",
    "labels = ['No Fraud', 'Fraud']\n",
    "print(classification_report(y_test, y_predRF, target_names=labels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build the Support Vector Machines model\n",
    "\n",
    "# Build and calculate the classifier's process\n",
    "start = time.time()\n",
    "clfSVM = svm.SVC()\n",
    "clfSVM.fit(X_trainfinal, Y_trainnew)  \n",
    "y_predSVM = clfSVM.predict(X_testfinal)\n",
    "end = time.time()\n",
    "print(\"This is the time of SVM = \", end - start)\n",
    "\n",
    "# Performance Metrics\n",
    "print('Support Vector Machines Metrics-Score:')\n",
    "#Confusion Matrix\n",
    "confusion_matrix4 = confusion_matrix(y_test, y_predSVM)\n",
    "print(\"\t\", \"pred no\", \"pred yes\")\n",
    "print(\"actual no\", confusion_matrix4[0])  \n",
    "print(\"actual yes\", confusion_matrix4[1])\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy_SVM = accuracy_score(y_test, y_predSVM)\n",
    "print('Accuracy: %f' % accuracy_SVM)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision_SVM = precision_score(y_test, y_predSVM)\n",
    "print('Precision: %f' % precision_SVM)\n",
    "\n",
    "#sys.exit()\n",
    "# recall: tp / (tp + fn)\n",
    "recall_SVM = recall_score(y_test, y_predSVM)\n",
    "print('Recall: %f' % recall_SVM)\n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1_SVM = f1_score(y_test, y_predSVM)\n",
    "print('F1 score: %f' % f1_SVM)\n",
    "\n",
    "#AUC_ROC\n",
    "auc_SVM = roc_auc_score(y_test, y_predSVM)\n",
    "print('ROC_AUC score: ', roc_auc_score(y_test, y_predSVM))\n",
    "\n",
    "# Classification report\n",
    "labels = ['No Fraud', 'Fraud']\n",
    "print(classification_report(y_test, y_predSVM, target_names=labels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# All ROC_AUC scores\n",
    "\n",
    "print('Logistic Regression ROC_AUC Score: ', roc_auc_score(y_test, y_predLR))\n",
    "print('Random Forests ROC_AUC Score: ', roc_auc_score(y_test, y_predRF))\n",
    "print('Naive Bayes ROC_AUC Score : ', roc_auc_score(y_test, y_predNB))\n",
    "print('Support Vector Machines ROC_AUC Score: ', roc_auc_score(y_test, y_predSVM))\n",
    "\n",
    "log_fpr, log_tpr, log_thresold = roc_curve(y_test, y_predLR)\n",
    "rf_fpr, rf_tpr, rf_threshold = roc_curve(y_test, y_predRF)\n",
    "nb_fpr, nb_tpr, nb_threshold = roc_curve(y_test, y_predNB)\n",
    "svm_fpr, svm_tpr, svm_threshold = roc_curve(y_test, y_predSVM)\n",
    "#ab_fpr, ab_tpr, ab_threshold = roc_curve(y_test, y_predAB)\n",
    "\n",
    "def graph_roc_curve_multiple(log_fpr, log_tpr, rf_fpr, rf_tpr, nb_fpr, nb_tpr, svm_fpr, svm_tpr):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('ROC Curve \\n Random Forests have the highest score', fontsize=18)\n",
    "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Score: {:.4f}'.format(roc_auc_score(y_test, y_predLR)))\n",
    "    plt.plot(rf_fpr, rf_tpr, label='Random Forests Score: {:.4f}'.format(roc_auc_score(y_test, y_predRF)))\n",
    "    plt.plot(nb_fpr, nb_tpr, label='Naive Bayes Score: {:.4f}'.format(roc_auc_score(y_test, y_predNB)))\n",
    "    plt.plot(svm_fpr, svm_tpr, label='Support Vector Machines Score: {:.4f}'.format(roc_auc_score(y_test, y_predSVM)))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.01, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
    "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
    "                )\n",
    "    plt.legend()\n",
    "\n",
    "graph_roc_curve_multiple(log_fpr, log_tpr, rf_fpr, rf_tpr, nb_fpr, nb_tpr, svm_fpr, svm_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
